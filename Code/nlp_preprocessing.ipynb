{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>mini sale day like black friday cyber monday g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2018-09-05</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>pretty sure like black friday liquidation sale...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>prime times amazon offered one day discount to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>black friday probably</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>cyber monday prime day also amazon amazing dea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49304</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>walmart</td>\n",
       "      <td>blitz used call black friday sales learned ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49305</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>walmart</td>\n",
       "      <td>dispel myth played cool said true wide open pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49306</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>walmart</td>\n",
       "      <td>yup problem capitalism monetary incentive give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49307</th>\n",
       "      <td>18</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>walmart</td>\n",
       "      <td>whole pallet pw crockpots leftover black frida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49308</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-14</td>\n",
       "      <td>walmart</td>\n",
       "      <td>yup specific happens often like black friday r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49309 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score retrieved_on subreddit_id  \\\n",
       "0          1   2018-09-03  amazonprime   \n",
       "1          1   2018-09-05  amazonprime   \n",
       "2          3   2018-09-18  amazonprime   \n",
       "3          2   2018-09-23  amazonprime   \n",
       "4          2   2018-09-23  amazonprime   \n",
       "...      ...          ...          ...   \n",
       "49304      1   2022-12-14      walmart   \n",
       "49305      5   2022-12-14      walmart   \n",
       "49306      5   2022-12-14      walmart   \n",
       "49307     18   2022-12-14      walmart   \n",
       "49308      2   2022-12-14      walmart   \n",
       "\n",
       "                                                    body  \n",
       "0      mini sale day like black friday cyber monday g...  \n",
       "1      pretty sure like black friday liquidation sale...  \n",
       "2      prime times amazon offered one day discount to...  \n",
       "3                                  black friday probably  \n",
       "4      cyber monday prime day also amazon amazing dea...  \n",
       "...                                                  ...  \n",
       "49304  blitz used call black friday sales learned ter...  \n",
       "49305  dispel myth played cool said true wide open pi...  \n",
       "49306  yup problem capitalism monetary incentive give...  \n",
       "49307  whole pallet pw crockpots leftover black frida...  \n",
       "49308  yup specific happens often like black friday r...  \n",
       "\n",
       "[49309 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Define the pattern to match your CSV files\n",
    "# Make sure to include *.csv to select only CSV files\n",
    "pattern = r'C:\\Users\\valhk\\Documents\\MMA\\Text analytics\\Team project\\cleaned posts\\*.csv'\n",
    "# Use glob to match all files following the pattern\n",
    "file_paths = glob.glob(pattern)\n",
    "output_file_path = r'C:\\Users\\valhk\\Documents\\MMA\\Text analytics\\Team project\\cleaned posts\\combined_blackfriday.csv'\n",
    "# Initialize an empty list to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the file paths and read each CSV file\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "    except PermissionError as e:\n",
    "        print(f\"Permission denied for file: {file_path}. Error: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into a single one, if any have been read\n",
    "if dfs:\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "else:\n",
    "    print(\"No CSV files read. Please check the directory path and permissions.\")\n",
    "combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Add year column\n",
    "combined_df['year'] = combined_df['retrieved_on'].str[:4]\n",
    "\n",
    "# Define your conditions and choices for the new column\n",
    "conditions = [\n",
    "    combined_df['year'] == '2018',\n",
    "    combined_df['year'] == '2019',\n",
    "    combined_df['year'] == '2020',\n",
    "    combined_df['year'] == '2021',\n",
    "    combined_df['year'] == '2022'\n",
    "]\n",
    "\n",
    "# Corresponding values for each condition\n",
    "choices = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Use np.select to apply these conditions and choices\n",
    "combined_df['var_year'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "# Now combined_df will have the 'var_year' column with values based on 'year'\n",
    "combined_df = combined_df.drop(columns=['year', 'retrieved_on'])\n",
    "original_df = combined_df\n",
    "original_df.to_csv('original_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\valhk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\valhk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\valhk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure the necessary NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert nltk POS tags to WordNet POS tags\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "# Function to lemmatize a sentence\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = pos_tag(word_tokenize(sentence))  \n",
    "    # Tuple of (word, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "# Create a copy of the DataFrame for lemmatization\n",
    "lemmatized_df = combined_df.copy()\n",
    "\n",
    "# Apply the lemmatization to the 'body' column\n",
    "lemmatized_df['body'] = lemmatized_df['body'].apply(lemmatize_sentence)\n",
    "\n",
    "lemmatized_df.to_csv('lemmatized_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "# Function to create a DataFrame with n-gram counts, using a sparse matrix\n",
    "def create_ngram_df(text_series, original_columns_df, ngram_range):\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        tokenizer=word_tokenize,\n",
    "        token_pattern=r'(?u)\\b[A-Za-z]+\\b',\n",
    "        max_features=max_features\n",
    "    )\n",
    "    X = vectorizer.fit_transform(text_series)\n",
    "    ngram_df = pd.DataFrame.sparse.from_spmatrix(X, columns=vectorizer.get_feature_names_out())\n",
    "    # Join the original columns with the n-gram DataFrame\n",
    "    final_df = pd.concat([original_columns_df.reset_index(drop=True), ngram_df], axis=1)\n",
    "    print(max_features)\n",
    "    return final_df\n",
    "\n",
    "# Tokenize using monogram, bigram, trigram, and (1,3) grams\n",
    "ngram_ranges = [(1, 1), (2, 2), (3, 3), (1, 3)]\n",
    "\n",
    "\n",
    "\n",
    "# Selecting the original columns to keep\n",
    "original_columns = combined_df[['score', 'subreddit_id', 'var_year']]\n",
    "lemmatized_columns = lemmatized_df[['score', 'subreddit_id', 'var_year']]\n",
    "\n",
    "# Create dictionaries for original and lemmatized dataframes with appropriate keys and including original columns\n",
    "original_ngram_dfs = {f\"{n[0]}_{n[1]}_grams_count\": create_ngram_df(combined_df['body'], original_columns, n) for n in ngram_ranges}\n",
    "lemmatized_ngram_dfs = {f\"{n[0]}_{n[1]}_grams_count\": create_ngram_df(lemmatized_df['body'], lemmatized_columns, n) for n in ngram_ranges}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "monograms_count_o = original_ngram_dfs['1_1_grams_count']\n",
    "bigrams_count_o = original_ngram_dfs['2_2_grams_count']\n",
    "trigrams_count_o = original_ngram_dfs['3_3_grams_count']\n",
    "one_three_grams_count_o = original_ngram_dfs['1_3_grams_count']\n",
    "\n",
    "monograms_count_l = lemmatized_ngram_dfs['1_1_grams_count']\n",
    "bigrams_count_l = lemmatized_ngram_dfs['2_2_grams_count']\n",
    "trigrams_count_l = lemmatized_ngram_dfs['3_3_grams_count']\n",
    "one_three_grams_count_l = lemmatized_ngram_dfs['1_3_grams_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\valhk\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# Custom tokenizer function\n",
    "def alpha_tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    alpha_tokens = [token for token in tokens if re.match(r'^[A-Za-z]+$', token)]\n",
    "    return alpha_tokens\n",
    "\n",
    "# Modified function to create a DataFrame with n-gram TF-IDF features and original columns\n",
    "def create_ngram_tfidf_df(text_series, original_columns_df, ngram_range, max_features=5000):\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        tokenizer=alpha_tokenizer,  # Use the custom tokenizer\n",
    "        max_features=max_features  # Limit the number of features\n",
    "    )\n",
    "    X = vectorizer.fit_transform(text_series)\n",
    "    ngram_df = pd.DataFrame.sparse.from_spmatrix(X, columns=vectorizer.get_feature_names_out())\n",
    "    # Join the original columns with the n-gram DataFrame\n",
    "    final_df = pd.concat([original_columns_df.reset_index(drop=True), ngram_df], axis=1)\n",
    "    return final_df\n",
    "\n",
    "# Assuming `combined_df` and `lemmatized_df` already have the 'score', 'subreddit_id', and 'var_year' columns\n",
    "original_columns = combined_df[['score', 'subreddit_id', 'var_year']]\n",
    "lemmatized_columns = lemmatized_df[['score', 'subreddit_id', 'var_year']]\n",
    "\n",
    "# Create dictionaries for original and lemmatized dataframes with appropriate keys and including original columns\n",
    "original_ngram_dfs_tfidf = {\n",
    "    f\"{n[0]}_{n[1]}_grams_tfidf\": create_ngram_tfidf_df(combined_df['body'], original_columns, n) for n in ngram_ranges\n",
    "}\n",
    "lemmatized_ngram_dfs_tfidf = {\n",
    "    f\"{n[0]}_{n[1]}_grams_tfidf\": create_ngram_tfidf_df(lemmatized_df['body'], lemmatized_columns, n) for n in ngram_ranges\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "monograms_count_tfidf_o = original_ngram_dfs_tfidf['1_1_grams_tfidf']\n",
    "bigrams_count_tfidf_o = original_ngram_dfs_tfidf['2_2_grams_tfidf']\n",
    "trigrams_count_tfidf_o = original_ngram_dfs_tfidf['3_3_grams_tfidf']\n",
    "one_three_grams_count_tfidf_o = original_ngram_dfs_tfidf['1_3_grams_tfidf']\n",
    "\n",
    "monograms_count_tfidf_l = lemmatized_ngram_dfs_tfidf['1_1_grams_tfidf']\n",
    "bigrams_count_tfidf_l = lemmatized_ngram_dfs_tfidf['2_2_grams_tfidf']\n",
    "trigrams_count_tfidf_l = lemmatized_ngram_dfs_tfidf['3_3_grams_tfidf']\n",
    "one_three_grams_count_tfidf_l = lemmatized_ngram_dfs_tfidf['1_3_grams_tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>var_year</th>\n",
       "      <th>able add cart</th>\n",
       "      <th>able find anything</th>\n",
       "      <th>able get away</th>\n",
       "      <th>able get deal</th>\n",
       "      <th>able get one</th>\n",
       "      <th>able get price</th>\n",
       "      <th>able order one</th>\n",
       "      <th>...</th>\n",
       "      <th>year work retail</th>\n",
       "      <th>year work thanksgiving</th>\n",
       "      <th>year year ago</th>\n",
       "      <th>year year year</th>\n",
       "      <th>yes best buy</th>\n",
       "      <th>yes black friday</th>\n",
       "      <th>yesterday black friday</th>\n",
       "      <th>youtu http youtu</th>\n",
       "      <th>youtube com watch</th>\n",
       "      <th>zelda breath wild</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>amazonprime</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49304</th>\n",
       "      <td>1</td>\n",
       "      <td>walmart</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49305</th>\n",
       "      <td>5</td>\n",
       "      <td>walmart</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49306</th>\n",
       "      <td>5</td>\n",
       "      <td>walmart</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49307</th>\n",
       "      <td>18</td>\n",
       "      <td>walmart</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49308</th>\n",
       "      <td>2</td>\n",
       "      <td>walmart</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49309 rows × 5003 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score subreddit_id  var_year  able add cart  able find anything  \\\n",
       "0          1  amazonprime         1            0.0                 0.0   \n",
       "1          1  amazonprime         1            0.0                 0.0   \n",
       "2          3  amazonprime         1            0.0                 0.0   \n",
       "3          2  amazonprime         1            0.0                 0.0   \n",
       "4          2  amazonprime         1            0.0                 0.0   \n",
       "...      ...          ...       ...            ...                 ...   \n",
       "49304      1      walmart         5            0.0                 0.0   \n",
       "49305      5      walmart         5            0.0                 0.0   \n",
       "49306      5      walmart         5            0.0                 0.0   \n",
       "49307     18      walmart         5            0.0                 0.0   \n",
       "49308      2      walmart         5            0.0                 0.0   \n",
       "\n",
       "       able get away  able get deal  able get one  able get price  \\\n",
       "0                0.0            0.0           0.0             0.0   \n",
       "1                0.0            0.0           0.0             0.0   \n",
       "2                0.0            0.0           0.0             0.0   \n",
       "3                0.0            0.0           0.0             0.0   \n",
       "4                0.0            0.0           0.0             0.0   \n",
       "...              ...            ...           ...             ...   \n",
       "49304            0.0            0.0           0.0             0.0   \n",
       "49305            0.0            0.0           0.0             0.0   \n",
       "49306            0.0            0.0           0.0             0.0   \n",
       "49307            0.0            0.0           0.0             0.0   \n",
       "49308            0.0            0.0           0.0             0.0   \n",
       "\n",
       "       able order one  ...  year work retail  year work thanksgiving  \\\n",
       "0                 0.0  ...               0.0                     0.0   \n",
       "1                 0.0  ...               0.0                     0.0   \n",
       "2                 0.0  ...               0.0                     0.0   \n",
       "3                 0.0  ...               0.0                     0.0   \n",
       "4                 0.0  ...               0.0                     0.0   \n",
       "...               ...  ...               ...                     ...   \n",
       "49304             0.0  ...               0.0                     0.0   \n",
       "49305             0.0  ...               0.0                     0.0   \n",
       "49306             0.0  ...               0.0                     0.0   \n",
       "49307             0.0  ...               0.0                     0.0   \n",
       "49308             0.0  ...               0.0                     0.0   \n",
       "\n",
       "       year year ago  year year year  yes best buy  yes black friday  \\\n",
       "0                0.0             0.0           0.0               0.0   \n",
       "1                0.0             0.0           0.0               0.0   \n",
       "2                0.0             0.0           0.0               0.0   \n",
       "3                0.0             0.0           0.0               0.0   \n",
       "4                0.0             0.0           0.0               0.0   \n",
       "...              ...             ...           ...               ...   \n",
       "49304            0.0             0.0           0.0               0.0   \n",
       "49305            0.0             0.0           0.0               0.0   \n",
       "49306            0.0             0.0           0.0               0.0   \n",
       "49307            0.0             0.0           0.0               0.0   \n",
       "49308            0.0             0.0           0.0               0.0   \n",
       "\n",
       "       yesterday black friday  youtu http youtu  youtube com watch  \\\n",
       "0                         0.0               0.0                0.0   \n",
       "1                         0.0               0.0                0.0   \n",
       "2                         0.0               0.0                0.0   \n",
       "3                         0.0               0.0                0.0   \n",
       "4                         0.0               0.0                0.0   \n",
       "...                       ...               ...                ...   \n",
       "49304                     0.0               0.0                0.0   \n",
       "49305                     0.0               0.0                0.0   \n",
       "49306                     0.0               0.0                0.0   \n",
       "49307                     0.0               0.0                0.0   \n",
       "49308                     0.0               0.0                0.0   \n",
       "\n",
       "       zelda breath wild  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "...                  ...  \n",
       "49304                0.0  \n",
       "49305                0.0  \n",
       "49306                0.0  \n",
       "49307                0.0  \n",
       "49308                0.0  \n",
       "\n",
       "[49309 rows x 5003 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_count_tfidf_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving monograms count DataFrames to CSV\n",
    "monograms_count_o.to_csv('monograms_count_original.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "monograms_count_l.to_csv('monograms_count_lemmatized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving bigrams count DataFrames to CSV\n",
    "bigrams_count_o.to_csv('bigrams_count_original.csv', index=False)\n",
    "bigrams_count_l.to_csv('bigrams_count_lemmatized.csv', index=False)\n",
    "\n",
    "# Saving trigrams count DataFrames to CSV\n",
    "trigrams_count_o.to_csv('trigrams_count_original.csv', index=False)\n",
    "trigrams_count_l.to_csv('trigrams_count_lemmatized.csv', index=False)\n",
    "\n",
    "# Saving one to three grams count DataFrames to CSV\n",
    "one_three_grams_count_o.to_csv('one_three_grams_count_original.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_three_grams_count_l.to_csv('one_three_grams_count_lemmatized.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Saving TF-IDF DataFrames to CSV\n",
    "monograms_count_tfidf_o.to_csv('monograms_count_tfidf_original.csv', index=False)\n",
    "monograms_count_tfidf_l.to_csv('monograms_count_tfidf_lemmatized.csv', index=False)\n",
    "bigrams_count_tfidf_o.to_csv('bigrams_count_tfidf_original.csv', index=False)\n",
    "bigrams_count_tfidf_l.to_csv('bigrams_count_tfidf_lemmatized.csv', index=False)\n",
    "trigrams_count_tfidf_o.to_csv('trigrams_count_tfidf_original.csv', index=False)\n",
    "trigrams_count_tfidf_l.to_csv('trigrams_count_tfidf_lemmatized.csv', index=False)\n",
    "one_three_grams_count_tfidf_o.to_csv('one_three_grams_count_tfidf_original.csv', index=False)\n",
    "one_three_grams_count_tfidf_l.to_csv('one_three_grams_count_tfidf_lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
